{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0yNfT5mq05-"
      },
      "source": [
        "#Simple Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTmTEPs-AOdo"
      },
      "source": [
        "#Training the Chatbot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqr8bvc4q8e8"
      },
      "source": [
        "##Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l3cvCLXq6uU"
      },
      "source": [
        "import random \n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "import tensorflow\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxHtx4HhsTvS"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from nltk.stem import WordNetLemmatizer   ##reducing to stem words\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD ##sgd (stochastic gradient descent)\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vSfT_uYtZkE"
      },
      "source": [
        "##Reading the JSON file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsxk-RzPth84"
      },
      "source": [
        "intents = json.loads(open(\"intents3.json\").read())"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpiUOKPPt56R",
        "outputId": "8e6002d1-c7fd-4b46-e59a-ae9a820db372"
      },
      "source": [
        "print(intents)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intents': [{'tag': 'greeting', 'patterns': ['Hi there', 'hi', 'How are you', 'Is anyone there?', 'Hey', 'Hola', 'Hello', 'Good day', 'how is it doing?', 'hello chatbot', 'hi buddy!', \"what's up?\"], 'responses': ['Hello, thanks for asking', 'Good to see you again', 'Hi there, how can I help?', 'hey!', 'hello', 'hi buddy', 'hi']}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye', 'Nice chatting to you, bye', 'Till next time', 'I am leaving', 'Bye Bye', 'See you Again', 'Ok bye', 'I was nice mwwt with you'], 'responses': ['See you!', 'Have a nice day', 'Bye! Come back again soon.', 'Sad to see you go :(', 'Talk to you later', 'Goodbye!', 'Bye buddy']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\", 'Awesome, thanks', 'Thanks for helping me', 'You are very helpful'], 'responses': ['Happy to help!', 'Any time!', 'My pleasure']}, {'tag': 'noanswer', 'patterns': [], 'responses': [\"Sorry, can't understand you\", 'Please give me more info', 'Not sure I understand']}, {'tag': 'options', 'patterns': ['How you could help me?', 'What you can do?', 'What help you provide?', 'How you can be helpful?'], 'responses': ['I can tell you which products our company QueenBeauty offer', 'We provide best service for beauty products']}, {'tag': 'beauty_products', 'patterns': ['What Products do you offer', 'What are your beauty products', 'Give me a list of your beauty products', 'List all beauty products offered', 'Which products are available', \"I'd like to buy something\", 'what are your products?', 'what do you sell?', 'what do you recommend?', 'what are you selling?', 'I would like to purchase something from your shop?'], 'responses': ['Beauty Cream, Skin Lightening Cream, Face Cream. Sunscreen, BB Cream, Skin Cleansing Cream, Barrier Cream and Collagen Cream are some of our products']}, {'tag': 'hours', 'patterns': ['when are you guys open', 'what are your hours', 'hours of operation', 'opening time'], 'responses': ['24/7']}, {'tag': 'company_and_assistant', 'patterns': ['what is your name', 'what should I call you', 'whats your name?', 'who are you?', 'Can you tell me your name?', 'tell me your name'], 'responses': ['hey, I am Beaut Assistant of Queen Beauty', \"I'm Beaut the assistant of floarin\", \"I'm Neural\", 'You can call me Neural']}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDl3pW2frgr7"
      },
      "source": [
        "##Cleaning the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_EYMhrxr5Gp"
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = [\"?\",\"!\",\".\",\",\",\"'\"]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9gxreFxvr-9",
        "outputId": "098d6424-c86b-4331-af28-9569652138bb"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOOi-4EkwKnE"
      },
      "source": [
        "for intent in intents['intents']:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    word_list = nltk.word_tokenize(pattern)   ## \"Hi there\"-->> [\"Hi\",\"there\"]\n",
        "    words.extend(word_list)\n",
        "    documents.append((word_list,intent[\"tag\"]))\n",
        "    if intent[\"tag\"] not in classes:\n",
        "      classes.append(intent[\"tag\"])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iZwckv0wlub",
        "outputId": "f8b22625-bb8a-4ac3-ce13-eb48dc44823f"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoAuPwkJxql3"
      },
      "source": [
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI5ln29RyVWq"
      },
      "source": [
        "words = sorted(set(words))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUjLb5hEymAH",
        "outputId": "2ac86b59-f45f-4f28-8f45-659c77fea2e7"
      },
      "source": [
        "classes = sorted(set(classes))\n",
        "print(classes)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['beauty_products', 'company_and_assistant', 'goodbye', 'greeting', 'hours', 'options', 'thanks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqSQPYeBy0AN"
      },
      "source": [
        "pickle.dump(words, open(\"words.pkl\", \"wb\"))\n",
        "pickle.dump(classes, open(\"classes.pkl\",\"wb\"))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW0_6vnez0M1"
      },
      "source": [
        "##Creating the bag of words model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9kUEcEJzzmD"
      },
      "source": [
        "training = []\n",
        "output_empty = [0]*len(classes)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QocQaswR0Bi2"
      },
      "source": [
        "for document in documents:\n",
        "  bag = []\n",
        "  word_patterns = document[0]\n",
        "  word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "  for word in words:\n",
        "    bag.append(1) if word in word_patterns else bag.append(0)\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(document[1])] = 1\n",
        "  training.append([bag, output_row])\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx2o46-izAgT"
      },
      "source": [
        "random.shuffle(training)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noQTu2w2zqme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75da9002-fc72-487b-b1de-971234b6f5b2"
      },
      "source": [
        "training = np.array(training)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I18oNdeR0JwG"
      },
      "source": [
        "train_X = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCLFyTOS1SVI"
      },
      "source": [
        "##Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dDerXzd1IsH"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_X[0]),),activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPLuXwmX8IcR",
        "outputId": "25817ca3-6e3f-487a-9c11-2a56477978f8"
      },
      "source": [
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov = True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u65Ss0wzGpOD"
      },
      "source": [
        "###Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikpaURd7-zeN",
        "outputId": "e554a79c-e8df-4662-9ee8-0e8d9d83e1f6"
      },
      "source": [
        "model.fit(np.array(train_X),np.array(train_y),epochs=200, batch_size=5, verbose=0)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fda5bac6790>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LWCdU38GvQj"
      },
      "source": [
        "###Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIGV1RLp_Vfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c644507-e4fe-46a6-aa9e-0aea24fdcf50"
      },
      "source": [
        "model.save(\"ChatbotProject\")\n",
        "print(\"Done\")\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ChatbotProject/assets\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvyAq0UkDiFL"
      },
      "source": [
        "#Testing the Chatbot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_bmgFetETRg"
      },
      "source": [
        "##Importing The Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16q6CUJrz33t"
      },
      "source": [
        "import nltk\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7S3zFNDEseK"
      },
      "source": [
        "##Loading the Data And Trained Chatbot Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paKzWJL70Sws"
      },
      "source": [
        "intents = json.loads(open(\"intents3.json\").read())\n",
        "words = pickle.load(open(\"words.pkl\",\"rb\"))\n",
        "classes = pickle.load(open(\"classes.pkl\",\"rb\"))\n",
        "model = load_model(\"ChatbotProject\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnifDHtoE_c-"
      },
      "source": [
        "##Function for Creating responses of questions asked to chtbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpSRos1ZEokb"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Eom9RQzD6yI"
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "  return sentence_words"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivs8CGG_EWko"
      },
      "source": [
        "def bag_of_words(sentence):\n",
        "\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  bag = [0]*len(words)\n",
        "  for w in sentence_words:\n",
        "    for i, word in enumerate(words):\n",
        "      if word==w:\n",
        "        bag[i] = 1\n",
        "  return np.array(bag)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qpQXFDVE53t"
      },
      "source": [
        "def predict_class(sentence):\n",
        "  bow = bag_of_words(sentence)\n",
        "  res = model.predict(np.array([bow]))[0]\n",
        "  ERROR_THRESHOLD = 0.25\n",
        "\n",
        "  results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "  results.sort(key=lambda x:x[1], reverse=True)\n",
        "  \n",
        "  return_list = []\n",
        "  for r in results:\n",
        "    return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
        "  return return_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq5aui-zzQPW"
      },
      "source": [
        "\n",
        "def response(sentence):\n",
        "    results = predict_class(sentence)\n",
        "    if results:\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                if i['tag'] == results[0][\"intent\"]:\n",
        "                    print(\"Chatbot- \",random.choice(i['responses']))\n",
        "                    if results[0][\"intent\"] ==\"goodbye\":\n",
        "                      return False\n",
        "                    else: \n",
        "                      return True\n",
        "            results.pop(0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVprN-8ODiri"
      },
      "source": [
        "##Ask To Chatbot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf2I256f6urT",
        "outputId": "6178412b-5aa8-4c4b-83e4-9052ed8598c9"
      },
      "source": [
        "answer = True\n",
        "while answer:\n",
        "    input_data = input(\"You- \")\n",
        "    answer = response(input_data)\n",
        " \n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You- hello chatbot\n",
            "Chatbot-  Hi there, how can I help?\n",
            "You- How can you help me?\n",
            "Chatbot-  I can tell you which products our company QueenBeauty offer\n",
            "You- whats your name?\n",
            "Chatbot-  hey, I am Beaut Assistant of Queen Beauty\n",
            "You- what products your company offer?\n",
            "Chatbot-  Beauty Cream, Skin Lightening Cream, Face Cream. Sunscreen, BB Cream, Skin Cleansing Cream, Barrier Cream and Collagen Cream are some of our products\n",
            "You- what are your service hours?\n",
            "Chatbot-  24/7\n",
            "You- Happy to meet you chatbot\n",
            "Chatbot-  See you!\n"
          ]
        }
      ]
    }
  ]
}